{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01502bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path to your dataset folder\n",
    "dataset_path = 'datos/'\n",
    "\n",
    "# Paths for the training and testing directories\n",
    "train_dir = './train'\n",
    "test_dir = './test'\n",
    "\n",
    "# Loop through each folder (class) in the dataset\n",
    "for folder in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "    # Check if it's a directory (class folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Get all images in the folder\n",
    "        images = os.listdir(folder_path)\n",
    "        image_paths = [os.path.join(folder_path, img) for img in images]\n",
    "\n",
    "        # Split the images into training and testing (80-20 split)\n",
    "        train_paths, test_paths = train_test_split(image_paths, test_size=0.2)\n",
    "\n",
    "        # Create class directories in train and test folders\n",
    "        os.makedirs(os.path.join(train_dir, folder), exist_ok=True)\n",
    "        os.makedirs(os.path.join(test_dir, folder), exist_ok=True)\n",
    "\n",
    "        # Move the files into train and test directories\n",
    "        for train_img in train_paths:\n",
    "            shutil.move(train_img, os.path.join(train_dir, folder, os.path.basename(train_img)))\n",
    "\n",
    "        for test_img in test_paths:\n",
    "            shutil.move(test_img, os.path.join(test_dir, folder, os.path.basename(test_img)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae19547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3680 images belonging to 46 classes.\n",
      "Found 920 images belonging to 46 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 110ms/step - accuracy: 0.3302 - loss: 2.5229 - val_accuracy: 0.8402 - val_loss: 0.4689\n",
      "Epoch 2/10\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 104ms/step - accuracy: 0.9277 - loss: 0.2477 - val_accuracy: 0.9924 - val_loss: 0.0357\n",
      "Epoch 3/10\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 101ms/step - accuracy: 0.9745 - loss: 0.0888 - val_accuracy: 0.9935 - val_loss: 0.0246\n",
      "Epoch 4/10\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 109ms/step - accuracy: 0.9845 - loss: 0.0469 - val_accuracy: 0.9957 - val_loss: 0.0136\n",
      "Epoch 5/10\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 146ms/step - accuracy: 0.9891 - loss: 0.0360 - val_accuracy: 0.9957 - val_loss: 0.0264\n",
      "Epoch 6/10\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 98ms/step - accuracy: 0.9910 - loss: 0.0330 - val_accuracy: 0.9946 - val_loss: 0.0267\n",
      "Epoch 7/10\n",
      "\u001b[1m 56/115\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.9899 - loss: 0.0261"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to the train and test directories (set these to where your data is stored)\n",
    "train_dir = './train'\n",
    "test_dir = './test'\n",
    "\n",
    "# Parameters\n",
    "img_height = 64  # Resize the image to 64x64\n",
    "img_width = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Data Preprocessing with ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,  # Normalize images to [0, 1]\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Load and preprocess images from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),  # Resize to 64x64\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # For multi-class classification\n",
    "    color_mode='grayscale'     # Grayscale images\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),  # Resize to 64x64\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "\n",
    "# Build the CNN Model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),  # Update input shape to match resized images\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),  # Adjusted Dense layer size after flattening\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # Multi-class classification loss\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,  # You can adjust the number of epochs\n",
    "    validation_data=test_generator\n",
    ")\n",
    "\n",
    "# Save the model after training\n",
    "model.save('trained_model.h5')\n",
    "\n",
    "# Save class indices from the training data generator for future use\n",
    "class_indices = train_generator.class_indices\n",
    "class_names = list(class_indices.keys())  # Get class names\n",
    "\n",
    "# Plotting the training history\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9dd0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el modelo \n",
    "\n",
    "model.save('japanese_vocab_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e000d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('japanese_vocab_model.h5')\n",
    "\n",
    "# Load class indices from the training phase (you can save this during training or hardcode it)\n",
    "class_names = ['aa', 'chi', 'ee']  # Replace with the actual class names from your training data\n",
    "\n",
    "# Function to preprocess the image and make a prediction\n",
    "def test_image(image_path):\n",
    "    # Load the image and resize it to the model's expected input size\n",
    "    img = image.load_img(image_path, target_size=(64, 64), color_mode='grayscale')\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    img_array = image.img_to_array(img)\n",
    "    \n",
    "    # Normalize the image\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    # Add an extra dimension to represent the batch size (1 image in this case)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Make a prediction\n",
    "    predictions = model.predict(img_array)\n",
    "    \n",
    "    # Get the class index with the highest probability\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    \n",
    "    # Get the predicted class name\n",
    "    predicted_class_name = class_names[predicted_class_index]\n",
    "    \n",
    "    # Display the image and predicted class\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f'Predicted: {predicted_class_name}')\n",
    "    plt.show()\n",
    "\n",
    "    return predicted_class_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c189b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002C28879D3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002C28879D3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'class_indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m image_path = \u001b[33m'\u001b[39m\u001b[33m./test/aa/drawing_20250805_081550.jpg\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m predicted_class = \u001b[43mtest_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicted class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtest_image\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m     29\u001b[39m predicted_class_index = np.argmax(predictions)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Get the class names from the model (this assumes the class names are in the model)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m class_names = \u001b[38;5;28mlist\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclass_indices\u001b[49m.keys())\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Get the predicted class name\u001b[39;00m\n\u001b[32m     35\u001b[39m predicted_class_name = class_names[predicted_class_index]\n",
      "\u001b[31mAttributeError\u001b[39m: 'Sequential' object has no attribute 'class_indices'"
     ]
    }
   ],
   "source": [
    "image_path = './test/aa/drawing_20250805_081550.jpg'\n",
    "predicted_class = test_image(image_path)\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
